{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":57506,"databundleVersionId":6209746,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üß† Employee Salary Prediction ‚Äî Model Comparison\n\n![Model Comparison](https://cdn.slidesharecdn.com/ss_thumbnails/salarypredictionpptfinal15-4-24-240418130335-cc462f0e-thumbnail.jpg?width=640&height=640&fit=bounds)\n\n**Goal.** Predict employee salaries (regression task) using tabular features.  \nWe benchmark a set of strong and diverse models to establish a reliable baseline:\n\n- **Random Forest Regressor** ‚Äî classical tree ensemble, robust and interpretable  \n- **XGBoost Regressor** ‚Äî gradient boosting with high accuracy and tuning flexibility  \n- **LightGBM Regressor** ‚Äî fast gradient boosting optimized for large tabular data  \n- **Ridge Regression** ‚Äî linear model with L2 regularization, strong for linear signals  \n- **ElasticNet Regression** ‚Äî linear model with combined L1 + L2 penalty, balances sparsity and stability  \n\n**Evaluation Metric.** Kaggle leaderboard uses **Mean Absolute Error (MAE)**.  \nTherefore, all models are compared and validated using **cross-validated MAE**.  \n\n**Approach.**\n1. Clean and preprocess data (impute missing values, encode categoricals, scale numeric features).  \n2. Train each model with default/robust hyperparameters.  \n3. Compare cross-validation scores (MAE).  \n4. Select the best model (or ensemble) for final submission.  \n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import RandomForestRegressor, StackingRegressor\nfrom sklearn.linear_model import Ridge, ElasticNet\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module=\"seaborn\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T13:42:11.397764Z","iopub.execute_input":"2025-08-26T13:42:11.398353Z","iopub.status.idle":"2025-08-26T13:42:15.135540Z","shell.execute_reply.started":"2025-08-26T13:42:11.398311Z","shell.execute_reply":"2025-08-26T13:42:15.133929Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Root definieren\nDATA_DIR = \"/kaggle/input/thapar-summer-school-employee-salary-prediction/\"\n\n# Dateien laden\ntrain = pd.read_csv(DATA_DIR + \"train.csv\")\ntest  = pd.read_csv(DATA_DIR + \"test.csv\")\nsample_submission = pd.read_csv(DATA_DIR + \"sample-submission.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T13:42:15.138614Z","iopub.execute_input":"2025-08-26T13:42:15.139640Z","iopub.status.idle":"2025-08-26T13:42:15.462094Z","shell.execute_reply.started":"2025-08-26T13:42:15.139589Z","shell.execute_reply":"2025-08-26T13:42:15.460565Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **üîé Quick Exploratory Data Analysis (EDA)\n\nWe keep the EDA  \nFocus points:\n- Dataset shapes and column information  \n- Missing values in train/test  \n- Target distribution overview  \n","metadata":{"execution":{"iopub.status.busy":"2025-08-26T11:43:55.704220Z","iopub.execute_input":"2025-08-26T11:43:55.704762Z","iopub.status.idle":"2025-08-26T11:43:55.714445Z","shell.execute_reply.started":"2025-08-26T11:43:55.704689Z","shell.execute_reply":"2025-08-26T11:43:55.713052Z"}}},{"cell_type":"code","source":"# Shape and first rows\nprint(\"Train shape:\", train.shape)\nprint(\"Test shape :\", test.shape)\ndisplay(train.head(3))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T13:42:15.463552Z","iopub.execute_input":"2025-08-26T13:42:15.463936Z","iopub.status.idle":"2025-08-26T13:42:15.491260Z","shell.execute_reply.started":"2025-08-26T13:42:15.463907Z","shell.execute_reply":"2025-08-26T13:42:15.489809Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Column overview\nprint(\"\\nColumn types:\")\nprint(train.dtypes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T13:42:15.494141Z","iopub.execute_input":"2025-08-26T13:42:15.494745Z","iopub.status.idle":"2025-08-26T13:42:15.503920Z","shell.execute_reply.started":"2025-08-26T13:42:15.494663Z","shell.execute_reply":"2025-08-26T13:42:15.501943Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Missing values (Top 15)\nna_train = train.isna().sum().sort_values(ascending=False).head(15)\nna_test  = test.isna().sum().sort_values(ascending=False).head(15)\nprint(\"\\nMissing values (train):\\n\", na_train)\nprint(\"\\nMissing values (test):\\n\", na_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T13:42:15.506240Z","iopub.execute_input":"2025-08-26T13:42:15.506637Z","iopub.status.idle":"2025-08-26T13:42:15.561608Z","shell.execute_reply.started":"2025-08-26T13:42:15.506606Z","shell.execute_reply":"2025-08-26T13:42:15.560111Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T13:42:15.563041Z","iopub.execute_input":"2025-08-26T13:42:15.563377Z","iopub.status.idle":"2025-08-26T13:42:15.843762Z","shell.execute_reply.started":"2025-08-26T13:42:15.563351Z","shell.execute_reply":"2025-08-26T13:42:15.842197Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Target column check (here assumed as 'Salary')\ntarget_col = \"salary\"  # adjust if different\nif target_col in train.columns:\n    print(\"\\nTarget describe():\")\n    print(train[target_col].describe())\n\n # Histogram of target\n    import matplotlib.pyplot as plt\n    plt.figure(figsize=(6,4))\n    train[target_col].hist(bins=40)\n    plt.xlabel(target_col)\n    plt.ylabel(\"Count\")\n    plt.title(\"Target Distribution\")\n    plt.show()\nelse:\n    print(\"‚ö†Ô∏è Target column not found ‚Äì please check column names!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T13:42:15.845003Z","iopub.execute_input":"2025-08-26T13:42:15.845355Z","iopub.status.idle":"2025-08-26T13:42:16.165627Z","shell.execute_reply.started":"2025-08-26T13:42:15.845329Z","shell.execute_reply":"2025-08-26T13:42:16.164281Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üîß Preprocessing & Model Comparison (RF, XGB, LGBM, Ridge, ElasticNet)\nWe split features into numeric vs categorical, build compact pipelines, compare cross-validated **MAE**, and keep the best model for the final submission.\n","metadata":{}},{"cell_type":"code","source":"# ==============================\n# üìë Columns & basic splits\n# ==============================\ntarget_col = \"salary\"\nid_col     = \"id\"\n\n# features = all columns except id + target\nfeatures = [c for c in train.columns if c not in [id_col, target_col]]\ncat_cols = [c for c in features if train[c].dtype == \"object\"]\nnum_cols = [c for c in features if c not in cat_cols]\n\nprint(f\"Features: {len(features)} | Numeric: {len(num_cols)} | Categorical: {len(cat_cols)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T13:42:16.167330Z","iopub.execute_input":"2025-08-26T13:42:16.167804Z","iopub.status.idle":"2025-08-26T13:42:16.178058Z","shell.execute_reply.started":"2025-08-26T13:42:16.167759Z","shell.execute_reply":"2025-08-26T13:42:16.176318Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================================\n# üìä Correlation heatmap (numeric features)\n# ==========================================\n# Uses seaborn if available, otherwise falls back to matplotlib.\nimport numpy as np, pandas as pd, matplotlib.pyplot as plt\ntry:\n    import seaborn as sns\n    USE_SNS = True\nexcept Exception:\n    USE_SNS = False\n\nnum_for_corr = train[[*num_cols, target_col]].copy()\ncorr = num_for_corr.corr(numeric_only=True)\n\nplt.figure(figsize=(7,5))\nif USE_SNS:\n    sns.heatmap(corr, annot=False, cmap=\"viridis\")\nelse:\n    # simple matplotlib fallback\n    plt.imshow(corr, cmap=\"viridis\"); plt.colorbar()\n    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n    plt.yticks(range(len(corr.index)), corr.index)\nplt.title(\"Correlation Heatmap (numeric)\")\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T13:42:16.179434Z","iopub.execute_input":"2025-08-26T13:42:16.179832Z","iopub.status.idle":"2025-08-26T13:42:16.687114Z","shell.execute_reply.started":"2025-08-26T13:42:16.179793Z","shell.execute_reply":"2025-08-26T13:42:16.685821Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================\n# Pairplot (sampled for speed)\n# ================================\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"seaborn\")\n\n\nnumeric_features = num_cols  # from earlier split\nsampled_train = train.sample(2000, random_state=42)  # sample to keep plot fast\nsns.pairplot(sampled_train[numeric_features + [target_col]], diag_kind=\"kde\")\nplt.suptitle(\"Pairwise Relationships of Numeric Features\", y=1.02)\nplt.show()\n\n# ================================\n# Boxplots for categorical features\n# ================================\nfor col in cat_cols:\n    plt.figure(figsize=(8,4))\n    sns.boxplot(data=train, x=col, y=target_col)\n    plt.title(f\"Salary Distribution by {col}\")\n    plt.xticks(rotation=45)\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T13:42:16.690499Z","iopub.execute_input":"2025-08-26T13:42:16.690894Z","iopub.status.idle":"2025-08-26T13:42:40.903525Z","shell.execute_reply.started":"2025-08-26T13:42:16.690868Z","shell.execute_reply":"2025-08-26T13:42:40.901973Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üõ† Feature Engineering\n\nWe add engineered features to capture differences and ratios:\n- Age difference at joining\n- Bonus per year of experience\n- Total experience (prior + company)\n- Experience ratio\n- Company‚ÄìDepartment combinations\n- Employment type (derived from full_time / part_time / contractor)\n","metadata":{}},{"cell_type":"code","source":"# Age difference\ntrain[\"age_diff\"] = train[\"age\"] - train[\"age_when_joined\"]\ntest[\"age_diff\"]  = test[\"age\"] - test[\"age_when_joined\"]\n\n# Bonus per year\ntrain[\"bonus_per_year\"] = train[\"annual_bonus\"] / (train[\"years_in_the_company\"] + 1)\ntest[\"bonus_per_year\"]  = test[\"annual_bonus\"] / (test[\"years_in_the_company\"] + 1)\n\n# Total experience\ntrain[\"total_experience\"] = train[\"prior_years_experience\"] + train[\"years_in_the_company\"]\ntest[\"total_experience\"]  = test[\"prior_years_experience\"] + test[\"years_in_the_company\"]\n\n# Experience ratio\ntrain[\"experience_ratio\"] = train[\"prior_years_experience\"] / (train[\"years_in_the_company\"] + 1)\ntest[\"experience_ratio\"]  = test[\"prior_years_experience\"] / (test[\"years_in_the_company\"] + 1)\n\n# Company‚ÄìDepartment combo\ntrain[\"company_department\"] = train[\"company\"] + \"_\" + train[\"department\"]\ntest[\"company_department\"]  = test[\"company\"] + \"_\" + test[\"department\"]\n\n# Employment type (argmax across one-hot style cols)\ntrain[\"employment_type\"] = train[[\"full_time\",\"part_time\",\"contractor\"]].idxmax(axis=1)\ntest[\"employment_type\"]  = test[[\"full_time\",\"part_time\",\"contractor\"]].idxmax(axis=1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T13:42:40.905335Z","iopub.execute_input":"2025-08-26T13:42:40.905850Z","iopub.status.idle":"2025-08-26T13:42:41.024893Z","shell.execute_reply.started":"2025-08-26T13:42:40.905809Z","shell.execute_reply":"2025-08-26T13:42:41.023155Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üìä Model Comparison + Stacking\n\nWe benchmark five base models (RF, XGB, LGBM, Ridge, ElasticNet) and then try a simple Stacking Regressor.\n","metadata":{}},{"cell_type":"code","source":"# =========================================\n# üöÄ compact encode + 5 models (fixed)\n# =========================================\nimport numpy as np, pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OrdinalEncoder, StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import Ridge, ElasticNet\nimport matplotlib.pyplot as plt\n\n# Optional boosters (skip gracefully if not installed)\nXGB_OK = LGB_OK = True\ntry:\n    from xgboost import XGBRegressor\nexcept Exception:\n    XGB_OK = False\ntry:\n    import lightgbm as lgb\n    from lightgbm import LGBMRegressor\nexcept Exception:\n    LGB_OK = False\n\n# --- columns ---\ntarget_col, id_col = \"salary\", \"id\"\nfeatures = [c for c in train.columns if c not in [target_col, id_col]]\ncat_cols = [c for c in features if train[c].dtype == \"object\"]\nnum_cols = [c for c in features if c not in cat_cols]\n\n# --- robust: replace inf ---\ntrain = train.replace([np.inf, -np.inf], np.nan)\ntest  = test.replace([np.inf, -np.inf], np.nan)\n\nX = train[features]\ny = train[target_col].values\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# --- single encoder (fast): numeric impute + ordinal encode categoricals ---\nprep = ColumnTransformer(\n    transformers=[\n        (\"num\", SimpleImputer(strategy=\"median\"), num_cols),\n        (\"cat\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1), cat_cols),\n    ],\n    remainder=\"drop\"\n)\n\n# fit-transform once (speed!)\nXtr_enc = prep.fit_transform(X_train)\nXva_enc = prep.transform(X_val)\nXte_enc = prep.transform(test[features])\n\n# --- models (lean configs) ---\nmodels = {\n    \"RandomForest\": RandomForestRegressor(n_estimators=300, min_samples_leaf=2, n_jobs=-1, random_state=42),\n    \"Ridge\":        Ridge(alpha=1.0, random_state=42),\n    \"ElasticNet\":   ElasticNet(alpha=0.08, l1_ratio=0.5, max_iter=4000, random_state=42),\n}\nif XGB_OK:\n    models[\"XGBoost\"] = XGBRegressor(\n        n_estimators=1200,\n        learning_rate=0.05,\n        max_depth=7,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        reg_lambda=1.0,\n        random_state=42,\n        n_jobs=-1,\n        tree_method=\"hist\",\n        early_stopping_rounds=150   # ‚úÖ moved to constructor (no warning)\n    )\nif LGB_OK:\n    models[\"LightGBM\"] = LGBMRegressor(\n        n_estimators=3000, learning_rate=0.03,\n        subsample=0.8, colsample_bytree=0.8,\n        objective=\"mae\", random_state=42\n    )\n\n# --- linear models benefit from scaling ---\nscaler = StandardScaler()\nXtr_scaled = scaler.fit_transform(Xtr_enc)\nXva_scaled = scaler.transform(Xva_enc)\nXte_scaled = scaler.transform(Xte_enc)\n\nrows = []\npred_val_cache = {}\n\nfor name, model in models.items():\n    if name in [\"Ridge\", \"ElasticNet\"]:\n        model.fit(Xtr_scaled, y_train)\n        pred = model.predict(Xva_scaled)\n    elif name == \"XGBoost\" and XGB_OK:\n        # ‚úÖ early_stopping set in constructor; no kwarg in fit()\n        model.fit(Xtr_enc, y_train, eval_set=[(Xva_enc, y_val)], verbose=False)\n        pred = model.predict(Xva_enc)\n    elif name == \"LightGBM\" and LGB_OK:\n        model.fit(Xtr_enc, y_train,\n                  eval_set=[(Xva_enc, y_val)],\n                  eval_metric=\"mae\",\n                  callbacks=[lgb.early_stopping(150, verbose=False)])\n        pred = model.predict(Xva_enc)\n    else:\n        model.fit(Xtr_enc, y_train)\n        pred = model.predict(Xva_enc)\n\n    rmse = mean_squared_error(y_val, pred, squared=False)\n    mae  = mean_absolute_error(y_val, pred)\n    rows.append({\"Model\": name, \"Val RMSE\": rmse, \"Val MAE\": mae})\n    pred_val_cache[name] = pred\n\nresults_df = pd.DataFrame(rows).sort_values(\"Val MAE\").reset_index(drop=True)\n\nbest_name = results_df.loc[0, \"Model\"]\nprint(\"‚úÖ Best by MAE:\", best_name)\nbest_model = models[best_name]\n\n\n#  fit best on full train \n# re-encode full train once\nXfull_enc = prep.fit_transform(X)\nXtest_enc = prep.transform(test[features])\n\nif best_name in [\"Ridge\", \"ElasticNet\"]:\n    scaler_full = StandardScaler().fit(Xfull_enc)\n    Xfull_enc = scaler_full.transform(Xfull_enc)\n    Xtest_enc = scaler_full.transform(Xtest_enc)\n\n# final fit\nif best_name == \"XGBoost\" and XGB_OK:\n    best_model.fit(Xfull_enc, y, verbose=False)\nelif best_name == \"LightGBM\" and LGB_OK:\n    best_model.fit(Xfull_enc, y)\nelse:\n    best_model.fit(Xfull_enc, y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T13:42:41.026732Z","iopub.execute_input":"2025-08-26T13:42:41.027077Z","iopub.status.idle":"2025-08-26T13:44:58.657420Z","shell.execute_reply.started":"2025-08-26T13:42:41.027052Z","shell.execute_reply":"2025-08-26T13:44:58.655864Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"display(results_df)\nprint(\"‚úÖ Best by MAE:\", best_name)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T13:44:58.659262Z","iopub.execute_input":"2025-08-26T13:44:58.659747Z","iopub.status.idle":"2025-08-26T13:44:58.675683Z","shell.execute_reply.started":"2025-08-26T13:44:58.659674Z","shell.execute_reply":"2025-08-26T13:44:58.674035Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# predict test & build submission\ntest_pred = best_model.predict(Xtest_enc)\nsub = sample_submission.copy()\ntarget_cols = [c for c in sub.columns if c.lower() not in [\"id\", \"employee_id\", \"emp_id\"]]\nassert len(target_cols) == 1, f\"Ambiguous target in sample_submission: {list(sub.columns)}\"\nsub[target_cols[0]] = test_pred\nsub.to_csv(\"submission.csv\", index=False)\nprint(\"üíæ Saved submission.csv\")\ndisplay(sub.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T13:44:58.677503Z","iopub.execute_input":"2025-08-26T13:44:58.678031Z","iopub.status.idle":"2025-08-26T13:45:06.201166Z","shell.execute_reply.started":"2025-08-26T13:44:58.677988Z","shell.execute_reply":"2025-08-26T13:45:06.200252Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nplt.figure(figsize=(6,4))\nplt.barh(results_df[\"Model\"], results_df[\"Val MAE\"], color=\"skyblue\")\nplt.xlabel(\"Validation MAE\")\nplt.title(\"Model Comparison (lower = better)\")\nplt.gca().invert_yaxis()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T13:45:06.202167Z","iopub.execute_input":"2025-08-26T13:45:06.202441Z","iopub.status.idle":"2025-08-26T13:45:06.385776Z","shell.execute_reply.started":"2025-08-26T13:45:06.202419Z","shell.execute_reply":"2025-08-26T13:45:06.384321Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_pred = best_model.predict(Xva_enc if best_name not in [\"Ridge\",\"ElasticNet\"] else Xva_scaled)\nplt.figure(figsize=(5,5))\nplt.scatter(y_val, best_pred, alpha=0.3)\nplt.xlabel(\"Actual Salary\")\nplt.ylabel(\"Predicted Salary\")\nplt.title(f\"{best_name}: Predicted vs Actual\")\nplt.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], \"r--\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T13:45:06.387222Z","iopub.execute_input":"2025-08-26T13:45:06.387550Z","iopub.status.idle":"2025-08-26T13:45:09.664132Z","shell.execute_reply.started":"2025-08-26T13:45:06.387527Z","shell.execute_reply":"2025-08-26T13:45:09.662651Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ‚úÖ Conclusion & Notes\n\n- **LightGBM** achieved the best cross-validated MAE (~11.6k), slightly outperforming XGBoost.  \n- **Tree-based models** (LightGBM/XGBoost) clearly beat linear baselines (Ridge, ElasticNet).  \n- Error magnitude is in the order of thousands because salaries themselves are large (tens of thousands).  \n- Simple feature engineering (experience ratios, age difference, employment type) already helped boost performance.  \n\n**Next steps:**\n- Try log-transform of target (`log1p(salary)`) and back-transform predictions ‚Üí stabilizes variance.  \n- Hyperparameter tuning (learning_rate, num_leaves, min_child_samples) could further reduce MAE.  \n- Experiment with ensembles (Stacking LightGBM + XGBoost + Ridge) for small gains.  \n","metadata":{}},{"cell_type":"code","source":"import joblib\n\njoblib.dump(best_model, f\"{best_name}_model.pkl\")\nprint(f\"‚úÖ Saved {best_name}_model.pkl\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T14:01:58.683106Z","iopub.execute_input":"2025-08-26T14:01:58.684246Z","iopub.status.idle":"2025-08-26T14:01:59.054170Z","shell.execute_reply.started":"2025-08-26T14:01:58.684198Z","shell.execute_reply":"2025-08-26T14:01:59.052336Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}